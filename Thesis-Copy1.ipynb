{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52ceb9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import and Defining functions complete\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "#Reads CSV file based on given filename\n",
    "def read_File(filename):\n",
    "  file = pd.read_csv(filename[0],sep=' ',skiprows=1,header=None)\n",
    "  file_loaded = file.iloc[:,[0,1,6]]\n",
    "  return file_loaded\n",
    "\n",
    "#Reads all the CSV files inside a folder\n",
    "def send_Features(folderName):\n",
    "  feature_List = []\n",
    "  for ii in [1,2,4,5,6]:\n",
    "    directory = folderName + '*' + str(ii) + '.svc'\n",
    "    feature = read_File(glob.glob(directory))\n",
    "    scaler = MinMaxScaler(feature_range=(0.00001,1))\n",
    "    scaled_feature = scaler.fit_transform(feature)\n",
    "    feature_List.append(scaled_feature)\n",
    "  return feature_List\n",
    "\n",
    "#Gets all the data from the folders\n",
    "def get_Folder(foldername, file_count, file_start=[1,1]):\n",
    "  feature_List = []\n",
    "  for ii in range(0,len(foldername)):\n",
    "    file_Directory = 'EMOTHAW/'+ foldername[ii]\n",
    "    for iii in range(file_start[ii],file_count[ii]): \n",
    "      if(iii < 10):\n",
    "        filename = file_Directory + '/user0000' + str(iii) + '/session00001/'\n",
    "      else:\n",
    "        filename = file_Directory + '/user000' + str(iii) + '/session00001/'\n",
    "      feature_List.append(send_Features(filename))\n",
    "  folder_num = 0\n",
    "  for amount in range(len(file_count)):\n",
    "    folder_num += file_count[amount] - file_start[amount]\n",
    "  for ii in range(folder_num):   \n",
    "    feature_List[ii] = keras.preprocessing.sequence.pad_sequences(feature_List[ii], padding='post', dtype='float32',maxlen=13000)\n",
    "    feature_List[ii] = np.hstack(feature_List[ii])  \n",
    "    feature_List[ii] = tf.squeeze(feature_List[ii]) \n",
    "  return (feature_List)\n",
    "    \n",
    "def get_Output():\n",
    "  scores = np.array(dass_Scores.values)\n",
    "  output = []\n",
    "  for score in scores:\n",
    "    if(score >= 21):\n",
    "      output.append(2)\n",
    "    elif(score >= 14): \n",
    "      output.append(2)\n",
    "    elif(score >= 10):\n",
    "      output.append(1)\n",
    "    else:\n",
    "      output.append(0)\n",
    "\n",
    "  return np.array(output)\n",
    "\n",
    "print(\"Import and Defining functions complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa7e0f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (129, 50, 1, 260, 15)\n",
      "The shape of Y is: (129,)\n",
      "The shape of Y is: (129, 1)\n",
      "{0: 1.29, 1: 0.645, 2: 0.43, 3: 0.4690909090909091, 4: 0.3225, 5: 0.516, 6: 0.5733333333333334, 7: 0.516, 8: 0.5733333333333334, 9: 0.86, 10: 1.29, 11: 1.032, 12: 1.032, 13: 5.16, 14: 2.58, 15: 2.58, 16: 2.58, 17: 1.72, 18: 5.16, 19: 2.58, 20: 2.58, 21: 5.16, 22: 5.16, 23: 5.16, 24: 2.58}\n"
     ]
    }
   ],
   "source": [
    "#Gets input values and then prints shape\n",
    "X = get_Folder(['Collection1','Collection2'],[46,85])\n",
    "X = tf.reshape(X,[129,50,1,260,15])\n",
    "print('The shape of X is:',X.shape)#X.shape)\n",
    "\n",
    "\n",
    "#Gets output values and prints shape\n",
    "dass_Scores = pd.read_csv('EMOTHAW/output.txt',sep=' ',header=None)\n",
    "Y = tf.squeeze(dass_Scores)\n",
    "print('The shape of Y is:',Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638c410e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, 50, 1, 260, 15)    0         \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 50, 1, 260, 30)    16320     \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 1, 260, 15)        8160      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3900)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               780200    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 25)                1275      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 831,131\n",
      "Trainable params: 831,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/60\n",
      "3/3 - 18s - loss: 90.0127 - acc: 0.0417 - val_loss: 99.4173 - val_acc: 0.0000e+00 - 18s/epoch - 6s/step\n",
      "Epoch 2/60\n",
      "3/3 - 11s - loss: 89.0794 - acc: 0.0417 - val_loss: 98.1468 - val_acc: 0.0000e+00 - 11s/epoch - 4s/step\n",
      "Epoch 3/60\n",
      "3/3 - 12s - loss: 87.7711 - acc: 0.0417 - val_loss: 96.3420 - val_acc: 0.0000e+00 - 12s/epoch - 4s/step\n",
      "Epoch 4/60\n",
      "3/3 - 12s - loss: 86.0012 - acc: 0.0417 - val_loss: 93.9561 - val_acc: 0.0000e+00 - 12s/epoch - 4s/step\n",
      "Epoch 5/60\n",
      "3/3 - 12s - loss: 83.6924 - acc: 0.0417 - val_loss: 90.7870 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 6/60\n",
      "3/3 - 11s - loss: 80.6118 - acc: 0.0625 - val_loss: 86.5430 - val_acc: 0.0303 - 11s/epoch - 4s/step\n",
      "Epoch 7/60\n",
      "3/3 - 11s - loss: 76.4618 - acc: 0.0729 - val_loss: 81.0706 - val_acc: 0.0303 - 11s/epoch - 4s/step\n",
      "Epoch 8/60\n",
      "3/3 - 12s - loss: 71.3036 - acc: 0.0729 - val_loss: 74.2134 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 9/60\n",
      "3/3 - 12s - loss: 64.9940 - acc: 0.0729 - val_loss: 65.8392 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 10/60\n",
      "3/3 - 12s - loss: 57.4187 - acc: 0.0729 - val_loss: 56.3653 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 11/60\n",
      "3/3 - 12s - loss: 49.3603 - acc: 0.0729 - val_loss: 47.0489 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 12/60\n",
      "3/3 - 13s - loss: 41.6615 - acc: 0.0729 - val_loss: 40.4740 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 13/60\n",
      "3/3 - 12s - loss: 38.9080 - acc: 0.0729 - val_loss: 39.3061 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 14/60\n",
      "3/3 - 12s - loss: 40.0034 - acc: 0.0729 - val_loss: 40.4277 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 15/60\n",
      "3/3 - 12s - loss: 40.9380 - acc: 0.0729 - val_loss: 39.6584 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 16/60\n",
      "3/3 - 12s - loss: 39.1855 - acc: 0.0729 - val_loss: 39.1240 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 17/60\n",
      "3/3 - 13s - loss: 38.6356 - acc: 0.0729 - val_loss: 39.1830 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 18/60\n",
      "3/3 - 12s - loss: 37.6964 - acc: 0.0729 - val_loss: 39.6624 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 19/60\n",
      "3/3 - 12s - loss: 37.6238 - acc: 0.0729 - val_loss: 40.0520 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 20/60\n",
      "3/3 - 12s - loss: 37.6266 - acc: 0.0729 - val_loss: 40.0103 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 21/60\n",
      "3/3 - 12s - loss: 37.4005 - acc: 0.0729 - val_loss: 39.6765 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 22/60\n",
      "3/3 - 13s - loss: 37.0904 - acc: 0.0729 - val_loss: 39.2475 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 23/60\n",
      "3/3 - 13s - loss: 36.7834 - acc: 0.0729 - val_loss: 38.9552 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 24/60\n",
      "3/3 - 12s - loss: 36.8111 - acc: 0.0729 - val_loss: 38.8223 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 25/60\n",
      "3/3 - 12s - loss: 36.6765 - acc: 0.0729 - val_loss: 38.8396 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 26/60\n",
      "3/3 - 12s - loss: 36.5299 - acc: 0.0729 - val_loss: 38.8766 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 27/60\n",
      "3/3 - 12s - loss: 36.4041 - acc: 0.0729 - val_loss: 38.9936 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 28/60\n",
      "3/3 - 12s - loss: 36.2710 - acc: 0.0729 - val_loss: 39.2074 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 29/60\n",
      "3/3 - 12s - loss: 36.1917 - acc: 0.0729 - val_loss: 39.3018 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 30/60\n",
      "3/3 - 12s - loss: 36.2749 - acc: 0.0729 - val_loss: 39.3810 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 31/60\n",
      "3/3 - 12s - loss: 36.0144 - acc: 0.0729 - val_loss: 39.1625 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 32/60\n",
      "3/3 - 12s - loss: 36.0022 - acc: 0.0729 - val_loss: 39.0048 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 33/60\n",
      "3/3 - 12s - loss: 35.9429 - acc: 0.0729 - val_loss: 39.0042 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 34/60\n",
      "3/3 - 12s - loss: 35.9249 - acc: 0.0729 - val_loss: 39.0426 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 35/60\n",
      "3/3 - 12s - loss: 35.8164 - acc: 0.0729 - val_loss: 39.0642 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 36/60\n",
      "3/3 - 12s - loss: 35.5948 - acc: 0.0729 - val_loss: 39.1360 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 37/60\n",
      "3/3 - 12s - loss: 35.4595 - acc: 0.0729 - val_loss: 39.2592 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 38/60\n",
      "3/3 - 12s - loss: 35.4151 - acc: 0.0729 - val_loss: 39.2908 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 39/60\n",
      "3/3 - 12s - loss: 35.3872 - acc: 0.0729 - val_loss: 39.3748 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 40/60\n",
      "3/3 - 13s - loss: 35.3319 - acc: 0.0729 - val_loss: 39.4592 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 41/60\n",
      "3/3 - 12s - loss: 35.2903 - acc: 0.0729 - val_loss: 39.4862 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 42/60\n",
      "3/3 - 12s - loss: 35.2634 - acc: 0.0729 - val_loss: 39.3672 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 43/60\n",
      "3/3 - 12s - loss: 35.0993 - acc: 0.0729 - val_loss: 39.2942 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 44/60\n",
      "3/3 - 14s - loss: 35.0842 - acc: 0.0729 - val_loss: 39.0896 - val_acc: 0.0303 - 14s/epoch - 5s/step\n",
      "Epoch 45/60\n",
      "3/3 - 12s - loss: 34.9045 - acc: 0.0729 - val_loss: 39.0217 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 46/60\n",
      "3/3 - 13s - loss: 34.8542 - acc: 0.0729 - val_loss: 39.0377 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 47/60\n",
      "3/3 - 12s - loss: 34.8174 - acc: 0.0729 - val_loss: 39.0596 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 48/60\n",
      "3/3 - 12s - loss: 34.6888 - acc: 0.0729 - val_loss: 39.1068 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 49/60\n",
      "3/3 - 12s - loss: 34.6409 - acc: 0.0729 - val_loss: 39.2285 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 50/60\n",
      "3/3 - 12s - loss: 34.5809 - acc: 0.0729 - val_loss: 39.3056 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 51/60\n",
      "3/3 - 12s - loss: 34.5082 - acc: 0.0729 - val_loss: 39.3155 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 52/60\n",
      "3/3 - 12s - loss: 34.4716 - acc: 0.0729 - val_loss: 39.3597 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 53/60\n",
      "3/3 - 12s - loss: 34.4140 - acc: 0.0729 - val_loss: 39.2558 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 54/60\n",
      "3/3 - 12s - loss: 34.2850 - acc: 0.0729 - val_loss: 39.2355 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 55/60\n",
      "3/3 - 13s - loss: 34.3015 - acc: 0.0729 - val_loss: 39.1756 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 56/60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mopt, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m---> 54\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(tf.keras.layers.Masking(mask_value=0, input_shape=(50,1,260,15)))\n",
    "model.add(keras.layers.ConvLSTM2D(filters=30, kernel_size=(1,3), activation='relu',padding=\"same\",return_sequences=True))\n",
    "model.add(keras.layers.ConvLSTM2D(filters=15, kernel_size=(1,3), activation='relu',padding=\"same\"))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(200, activation='relu'))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(50, activation='relu'))\n",
    "model.add(keras.layers.Dense(25, activation='relu'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=700,\n",
    "    decay_rate=0.5)\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['acc'])\n",
    "model.summary()\n",
    "model.fit(X, Y, validation_split=0.25, epochs=60, batch_size=40, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c449b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 3s 599ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Real</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>6.664570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.920356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>7.284317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>6.814342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.747361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>9.853558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>7.137829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>6.153368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>7.089871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>7.618398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>7.065090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>24</td>\n",
       "      <td>4.585220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5</td>\n",
       "      <td>9.240062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>8.073075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5</td>\n",
       "      <td>7.251368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>6.792411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>5.895766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>8.904449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>8.571370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7</td>\n",
       "      <td>5.651257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>8.995650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2</td>\n",
       "      <td>7.060582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>6.076176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>6.595597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6</td>\n",
       "      <td>6.086342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6</td>\n",
       "      <td>6.903368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>6.243125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>5.734635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>16</td>\n",
       "      <td>9.740758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>5.973177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>7.379095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>7.509308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>25</td>\n",
       "      <td>10.896706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>7.940401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>6.604462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>21</td>\n",
       "      <td>8.262593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>6.097290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>13</td>\n",
       "      <td>7.470760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>7</td>\n",
       "      <td>5.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>9</td>\n",
       "      <td>5.630533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8</td>\n",
       "      <td>7.956967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>15</td>\n",
       "      <td>8.471487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>13</td>\n",
       "      <td>7.931628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>7</td>\n",
       "      <td>6.684175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>6.925889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>8</td>\n",
       "      <td>8.664406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20</td>\n",
       "      <td>14.600411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>6.702670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>11</td>\n",
       "      <td>10.178158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>6.569198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>6</td>\n",
       "      <td>10.334436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>7</td>\n",
       "      <td>7.773973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>5.657572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>7</td>\n",
       "      <td>10.644433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8</td>\n",
       "      <td>8.679605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2</td>\n",
       "      <td>6.150913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>8.100587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>15</td>\n",
       "      <td>7.339744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>9</td>\n",
       "      <td>6.454429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4</td>\n",
       "      <td>7.090856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5</td>\n",
       "      <td>5.768585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>18</td>\n",
       "      <td>7.710701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2</td>\n",
       "      <td>5.328699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>9</td>\n",
       "      <td>8.364905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0</td>\n",
       "      <td>5.946592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1</td>\n",
       "      <td>8.361598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>6</td>\n",
       "      <td>7.064221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>6.562602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4</td>\n",
       "      <td>7.289512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>21</td>\n",
       "      <td>8.752502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>11</td>\n",
       "      <td>8.071083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>7.681775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3</td>\n",
       "      <td>7.164367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>6</td>\n",
       "      <td>9.910055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0</td>\n",
       "      <td>5.727498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7</td>\n",
       "      <td>6.690063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4</td>\n",
       "      <td>7.324162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>6.187757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2</td>\n",
       "      <td>7.558333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>13</td>\n",
       "      <td>5.971952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>23</td>\n",
       "      <td>7.542681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4</td>\n",
       "      <td>6.333348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>3</td>\n",
       "      <td>7.997322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>11</td>\n",
       "      <td>8.212417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0</td>\n",
       "      <td>6.428067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>3</td>\n",
       "      <td>6.169216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>8</td>\n",
       "      <td>7.798076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>7</td>\n",
       "      <td>7.606872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5</td>\n",
       "      <td>6.973347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>4</td>\n",
       "      <td>8.092066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>4</td>\n",
       "      <td>7.567323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>9</td>\n",
       "      <td>6.813848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>22</td>\n",
       "      <td>9.542391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0</td>\n",
       "      <td>6.558579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>5.117622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>8</td>\n",
       "      <td>6.173801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4</td>\n",
       "      <td>7.887868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>6.545968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>10.259526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2</td>\n",
       "      <td>5.630359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>2</td>\n",
       "      <td>7.934935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>14</td>\n",
       "      <td>5.533075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>8</td>\n",
       "      <td>7.223048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>8</td>\n",
       "      <td>6.137624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>2</td>\n",
       "      <td>5.139381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3</td>\n",
       "      <td>7.634575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2</td>\n",
       "      <td>6.794413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>3</td>\n",
       "      <td>7.765147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2</td>\n",
       "      <td>6.521725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>25</td>\n",
       "      <td>8.353929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>3</td>\n",
       "      <td>9.264060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>17</td>\n",
       "      <td>8.822431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>7</td>\n",
       "      <td>6.980267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>7</td>\n",
       "      <td>6.741547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>18</td>\n",
       "      <td>7.580142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>4</td>\n",
       "      <td>6.516062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>9</td>\n",
       "      <td>6.237650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>7</td>\n",
       "      <td>6.719127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>5</td>\n",
       "      <td>5.906114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>4</td>\n",
       "      <td>6.834573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2</td>\n",
       "      <td>6.514648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5</td>\n",
       "      <td>6.853001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>19</td>\n",
       "      <td>6.449782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>10</td>\n",
       "      <td>8.493877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>10</td>\n",
       "      <td>7.918555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>18</td>\n",
       "      <td>7.361453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>20</td>\n",
       "      <td>7.904401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6</td>\n",
       "      <td>9.601088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6</td>\n",
       "      <td>6.489285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Real  Predicted\n",
       "0       2   6.664570\n",
       "1       1   6.920356\n",
       "2       5   7.284317\n",
       "3      13   6.814342\n",
       "4       5   7.747361\n",
       "5      11   9.853558\n",
       "6       8   7.137829\n",
       "7       3   6.153368\n",
       "8       8   7.089871\n",
       "9      17   7.618398\n",
       "10     10   7.065090\n",
       "11     24   4.585220\n",
       "12      5   9.240062\n",
       "13     16   8.073075\n",
       "14      5   7.251368\n",
       "15     13   6.792411\n",
       "16      4   5.895766\n",
       "17     10   8.904449\n",
       "18      9   8.571370\n",
       "19      7   5.651257\n",
       "20     11   8.995650\n",
       "21      2   7.060582\n",
       "22      6   6.076176\n",
       "23      2   6.595597\n",
       "24      6   6.086342\n",
       "25      6   6.903368\n",
       "26      4   6.243125\n",
       "27      4   5.734635\n",
       "28     16   9.740758\n",
       "29      4   5.973177\n",
       "30      4   7.379095\n",
       "31      4   7.509308\n",
       "32     25  10.896706\n",
       "33      5   7.940401\n",
       "34      5   6.604462\n",
       "35     21   8.262593\n",
       "36      6   6.097290\n",
       "37     13   7.470760\n",
       "38      7   5.296875\n",
       "39      9   5.630533\n",
       "40      8   7.956967\n",
       "41     15   8.471487\n",
       "42     13   7.931628\n",
       "43      7   6.684175\n",
       "44      3   6.925889\n",
       "45      8   8.664406\n",
       "46     20  14.600411\n",
       "47      3   6.702670\n",
       "48     11  10.178158\n",
       "49      3   6.569198\n",
       "50      6  10.334436\n",
       "51      7   7.773973\n",
       "52      3   5.657572\n",
       "53      7  10.644433\n",
       "54      8   8.679605\n",
       "55      2   6.150913\n",
       "56      1   8.100587\n",
       "57     15   7.339744\n",
       "58      9   6.454429\n",
       "59      4   7.090856\n",
       "60      5   5.768585\n",
       "61     18   7.710701\n",
       "62      2   5.328699\n",
       "63      9   8.364905\n",
       "64      0   5.946592\n",
       "65      1   8.361598\n",
       "66      6   7.064221\n",
       "67      1   6.562602\n",
       "68      4   7.289512\n",
       "69     21   8.752502\n",
       "70     11   8.071083\n",
       "71      1   7.681775\n",
       "72      3   7.164367\n",
       "73      6   9.910055\n",
       "74      0   5.727498\n",
       "75      7   6.690063\n",
       "76      4   7.324162\n",
       "77      1   6.187757\n",
       "78      2   7.558333\n",
       "79     13   5.971952\n",
       "80     23   7.542681\n",
       "81      4   6.333348\n",
       "82      3   7.997322\n",
       "83     11   8.212417\n",
       "84      0   6.428067\n",
       "85      3   6.169216\n",
       "86      8   7.798076\n",
       "87      7   7.606872\n",
       "88      5   6.973347\n",
       "89      4   8.092066\n",
       "90      4   7.567323\n",
       "91      9   6.813848\n",
       "92     22   9.542391\n",
       "93      0   6.558579\n",
       "94      1   5.117622\n",
       "95      8   6.173801\n",
       "96      4   7.887868\n",
       "97      4   6.545968\n",
       "98      1  10.259526\n",
       "99      2   5.630359\n",
       "100     2   7.934935\n",
       "101    14   5.533075\n",
       "102     8   7.223048\n",
       "103     8   6.137624\n",
       "104     2   5.139381\n",
       "105     3   7.634575\n",
       "106     2   6.794413\n",
       "107     3   7.765147\n",
       "108     2   6.521725\n",
       "109    25   8.353929\n",
       "110     3   9.264060\n",
       "111    17   8.822431\n",
       "112     7   6.980267\n",
       "113     7   6.741547\n",
       "114    18   7.580142\n",
       "115     4   6.516062\n",
       "116     9   6.237650\n",
       "117     7   6.719127\n",
       "118     5   5.906114\n",
       "119     4   6.834573\n",
       "120     2   6.514648\n",
       "121     5   6.853001\n",
       "122    19   6.449782\n",
       "123    10   8.493877\n",
       "124    10   7.918555\n",
       "125    18   7.361453\n",
       "126    20   7.904401\n",
       "127     6   9.601088\n",
       "128     6   6.489285"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.predict(X)\n",
    "output = tf.squeeze(output)\n",
    "\n",
    "adjusted = output\n",
    "frame = pd.DataFrame({'Real':tf.squeeze(Y),'Predicted':output},index=range(129))\n",
    "pd.set_option('display.max_rows', None)\n",
    "frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bb7e8fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 - 11s - loss: 34.1367 - acc: 0.0729 - val_loss: 39.2014 - val_acc: 0.0303 - 11s/epoch - 4s/step\n",
      "Epoch 2/50\n",
      "3/3 - 11s - loss: 34.2288 - acc: 0.0729 - val_loss: 39.3718 - val_acc: 0.0303 - 11s/epoch - 4s/step\n",
      "Epoch 3/50\n",
      "3/3 - 12s - loss: 34.0713 - acc: 0.0729 - val_loss: 39.4194 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 4/50\n",
      "3/3 - 12s - loss: 34.0373 - acc: 0.0729 - val_loss: 39.3603 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 5/50\n",
      "3/3 - 12s - loss: 33.9081 - acc: 0.0729 - val_loss: 39.2189 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 6/50\n",
      "3/3 - 12s - loss: 33.8425 - acc: 0.0729 - val_loss: 39.0522 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 7/50\n",
      "3/3 - 12s - loss: 33.8314 - acc: 0.0729 - val_loss: 39.0348 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 8/50\n",
      "3/3 - 12s - loss: 33.6505 - acc: 0.0729 - val_loss: 39.0924 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 9/50\n",
      "3/3 - 12s - loss: 33.5723 - acc: 0.0729 - val_loss: 39.2571 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 10/50\n",
      "3/3 - 12s - loss: 33.6506 - acc: 0.0729 - val_loss: 39.5154 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 11/50\n",
      "3/3 - 12s - loss: 33.6017 - acc: 0.0729 - val_loss: 39.4618 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 12/50\n",
      "3/3 - 12s - loss: 33.5218 - acc: 0.0729 - val_loss: 39.3380 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 13/50\n",
      "3/3 - 12s - loss: 33.3210 - acc: 0.0729 - val_loss: 39.0530 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 14/50\n",
      "3/3 - 12s - loss: 33.1981 - acc: 0.0729 - val_loss: 38.9713 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 15/50\n",
      "3/3 - 12s - loss: 33.2186 - acc: 0.0729 - val_loss: 38.9485 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 16/50\n",
      "3/3 - 12s - loss: 33.2362 - acc: 0.0729 - val_loss: 38.9668 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 17/50\n",
      "3/3 - 12s - loss: 33.1013 - acc: 0.0729 - val_loss: 39.1345 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 18/50\n",
      "3/3 - 12s - loss: 32.9665 - acc: 0.0729 - val_loss: 39.3742 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 19/50\n",
      "3/3 - 12s - loss: 33.0708 - acc: 0.0729 - val_loss: 39.5128 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 20/50\n",
      "3/3 - 12s - loss: 32.9407 - acc: 0.0729 - val_loss: 39.3204 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 21/50\n",
      "3/3 - 12s - loss: 32.8881 - acc: 0.0729 - val_loss: 39.0555 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 22/50\n",
      "3/3 - 12s - loss: 32.6767 - acc: 0.0729 - val_loss: 39.0177 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 23/50\n",
      "3/3 - 12s - loss: 32.6587 - acc: 0.0729 - val_loss: 39.0327 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 24/50\n",
      "3/3 - 12s - loss: 32.5742 - acc: 0.0729 - val_loss: 39.0683 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 25/50\n",
      "3/3 - 12s - loss: 32.4532 - acc: 0.0729 - val_loss: 39.1280 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 26/50\n",
      "3/3 - 12s - loss: 32.3534 - acc: 0.0729 - val_loss: 39.2517 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 27/50\n",
      "3/3 - 12s - loss: 32.2781 - acc: 0.0729 - val_loss: 39.3796 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 28/50\n",
      "3/3 - 12s - loss: 32.2892 - acc: 0.0729 - val_loss: 39.5990 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 29/50\n",
      "3/3 - 12s - loss: 32.3136 - acc: 0.0729 - val_loss: 39.5612 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 30/50\n",
      "3/3 - 12s - loss: 32.1201 - acc: 0.0729 - val_loss: 39.3121 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 31/50\n",
      "3/3 - 12s - loss: 31.9661 - acc: 0.0729 - val_loss: 39.1814 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 32/50\n",
      "3/3 - 12s - loss: 31.8225 - acc: 0.0729 - val_loss: 39.1300 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 33/50\n",
      "3/3 - 12s - loss: 31.7415 - acc: 0.0729 - val_loss: 39.0875 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 34/50\n",
      "3/3 - 12s - loss: 31.8538 - acc: 0.0729 - val_loss: 39.1120 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 35/50\n",
      "3/3 - 12s - loss: 31.7225 - acc: 0.0729 - val_loss: 39.1051 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 36/50\n",
      "3/3 - 12s - loss: 31.5834 - acc: 0.0729 - val_loss: 39.0949 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 37/50\n",
      "3/3 - 12s - loss: 31.2942 - acc: 0.0729 - val_loss: 39.1089 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 38/50\n",
      "3/3 - 12s - loss: 31.2315 - acc: 0.0729 - val_loss: 39.2790 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 39/50\n",
      "3/3 - 12s - loss: 31.0670 - acc: 0.0729 - val_loss: 39.2733 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 40/50\n",
      "3/3 - 12s - loss: 31.0660 - acc: 0.0729 - val_loss: 39.1283 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 41/50\n",
      "3/3 - 12s - loss: 30.7793 - acc: 0.0729 - val_loss: 39.2462 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 42/50\n",
      "3/3 - 12s - loss: 30.7168 - acc: 0.0729 - val_loss: 39.2556 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 43/50\n",
      "3/3 - 12s - loss: 30.6145 - acc: 0.0729 - val_loss: 39.1287 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 44/50\n",
      "3/3 - 12s - loss: 30.4195 - acc: 0.0729 - val_loss: 39.0588 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 45/50\n",
      "3/3 - 13s - loss: 30.2087 - acc: 0.0729 - val_loss: 38.9709 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 46/50\n",
      "3/3 - 13s - loss: 30.0365 - acc: 0.0729 - val_loss: 38.9197 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 47/50\n",
      "3/3 - 12s - loss: 29.9005 - acc: 0.0729 - val_loss: 38.9714 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 48/50\n",
      "3/3 - 13s - loss: 29.7865 - acc: 0.0729 - val_loss: 39.1137 - val_acc: 0.0303 - 13s/epoch - 4s/step\n",
      "Epoch 49/50\n",
      "3/3 - 12s - loss: 29.6061 - acc: 0.0729 - val_loss: 39.3507 - val_acc: 0.0303 - 12s/epoch - 4s/step\n",
      "Epoch 50/50\n",
      "3/3 - 12s - loss: 29.4772 - acc: 0.0729 - val_loss: 39.5251 - val_acc: 0.0303 - 12s/epoch - 4s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21989917b80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=3e-6,\n",
    "    decay_steps=400,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.fit(X, Y, validation_split=0.25, epochs=50, batch_size=40, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fecce91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
